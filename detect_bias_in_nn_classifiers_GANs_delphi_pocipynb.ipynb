{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTRjQxYuvunQ4L02aHBC+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericyoc/detect_bias_in_nn_classifiers_GANs_delphi_poc/blob/main/detect_bias_in_nn_classifiers_GANs_delphi_pocipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install wget"
      ],
      "metadata": {
        "id": "oUYyUzPtWMOK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XfoHikBPWEVz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from transformers import TFBertForSequenceClassification, TFDistilBertForSequenceClassification, BertTokenizer, DistilBertTokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import wget\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Research-based thresholds\n",
        "THRESHOLDS = {\n",
        "    'gender': {'unbiased': 0.25, 'biased': 0.5},\n",
        "    'racial': {'unbiased': 0.3, 'biased': 0.6},\n",
        "    'age': {'unbiased': 0.2, 'biased': 0.45}\n",
        "}"
      ],
      "metadata": {
        "id": "pq9axwGiG8KT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapeAdapter(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        super(ShapeAdapter, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.output_dim])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.kernel)"
      ],
      "metadata": {
        "id": "vQdkb8JhHDqm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_datasets():\n",
        "    urls = {\n",
        "        'gender_bias': 'https://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat',\n",
        "        'racial_bias': 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data',\n",
        "        'age_bias': 'https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv'\n",
        "    }\n",
        "    for bias_type, url in urls.items():\n",
        "        filename = f'{bias_type}_data.csv'\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"Downloading {bias_type} dataset...\")\n",
        "            wget.download(url, filename)\n",
        "            print(f\"\\n{bias_type} dataset downloaded successfully.\")\n",
        "        else:\n",
        "            print(f\"{bias_type} dataset already exists.\")"
      ],
      "metadata": {
        "id": "wtaWt0u-HHAE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filename, bias_type):\n",
        "    print(f\"Loading and preprocessing {bias_type} data...\")\n",
        "    if bias_type == 'gender_bias':\n",
        "        data = pd.read_csv(filename, sep='\\s+', header=None, skiprows=1)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = (data.iloc[:, -1] == 'F').astype(int).values\n",
        "    elif bias_type == 'racial_bias':\n",
        "        data = pd.read_csv(filename, sep=' ', header=None)\n",
        "        for col in data.columns:\n",
        "            if data[col].dtype == 'object':\n",
        "                le = LabelEncoder()\n",
        "                data[col] = le.fit_transform(data[col].astype(str))\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = (data.iloc[:, -1] == 2).astype(int).values\n",
        "    else:  # age_bias\n",
        "        data = pd.read_csv(filename)\n",
        "        X = data.drop(['age', 'DEATH_EVENT'], axis=1).values\n",
        "        y = (data['age'] > 60).astype(int).values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    print(f\"{bias_type} data preprocessed. Shape: {X.shape}\")\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "Zpk2T4ytHKm9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    print(\"Loading pre-trained models...\")\n",
        "    low_bias_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    high_bias_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    return low_bias_model, high_bias_model, bert_tokenizer, distilbert_tokenizer"
      ],
      "metadata": {
        "id": "vL-rF25oHNqC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gan(input_shape, bert_output_shape):\n",
        "    generator = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(input_shape, activation='tanh')\n",
        "    ])\n",
        "\n",
        "    discriminator = tf.keras.Sequential([\n",
        "        ShapeAdapter(256),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    return generator, discriminator"
      ],
      "metadata": {
        "id": "cdh924RAHQX0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, X_train, epochs=1000, batch_size=32):\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        generated_data = generator.predict(noise, verbose=0)\n",
        "        real_data = X_train[np.random.randint(0, X_train.shape[0], batch_size)]\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))\n",
        "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        g_loss = generator.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            tqdm.write(f\"Epoch {epoch}, D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\")"
      ],
      "metadata": {
        "id": "I5LbsYDgHTA1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_bias(model, tokenizer, gans, test_sentences):\n",
        "    inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "    outputs = model(inputs)\n",
        "    predictions = tf.nn.softmax(outputs.logits, axis=-1).numpy()\n",
        "\n",
        "    bias_scores = []\n",
        "\n",
        "    for gan_name, (generator, discriminator) in gans.items():\n",
        "        noise = np.random.normal(0, 1, (len(test_sentences), 100))\n",
        "        generated_data = generator.predict(noise, verbose=0)\n",
        "\n",
        "        # Ensure predictions match the discriminator's expected input shape\n",
        "        if predictions.shape[1] != generated_data.shape[1]:\n",
        "            if predictions.shape[1] < generated_data.shape[1]:\n",
        "                padding = np.zeros((predictions.shape[0], generated_data.shape[1] - predictions.shape[1]))\n",
        "                predictions = np.hstack([predictions, padding])\n",
        "            else:\n",
        "                predictions = predictions[:, :generated_data.shape[1]]\n",
        "\n",
        "        real_scores = discriminator.predict(predictions)\n",
        "        fake_scores = discriminator.predict(generated_data)\n",
        "\n",
        "        bias_score = np.mean(real_scores) - np.mean(fake_scores)\n",
        "        bias_scores.append((gan_name, bias_score))\n",
        "\n",
        "    return bias_scores"
      ],
      "metadata": {
        "id": "AJl2Jq-qHVXU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_bias_scores(bias_scores):\n",
        "    interpretations = []\n",
        "    overall_bias = \"unbiased\"\n",
        "\n",
        "    for bias_type, score in bias_scores:\n",
        "        if abs(score) <= THRESHOLDS[bias_type]['unbiased']:\n",
        "            interpretation = f\"{bias_type.capitalize()} bias: Unbiased (score: {score:.4f})\"\n",
        "        elif abs(score) >= THRESHOLDS[bias_type]['biased']:\n",
        "            interpretation = f\"{bias_type.capitalize()} bias: Significantly biased (score: {score:.4f})\"\n",
        "            overall_bias = \"biased\"\n",
        "        else:\n",
        "            interpretation = f\"{bias_type.capitalize()} bias: Moderately biased (score: {score:.4f})\"\n",
        "            if overall_bias == \"unbiased\":\n",
        "                overall_bias = \"moderately biased\"\n",
        "\n",
        "        interpretations.append(interpretation)\n",
        "\n",
        "    explanation = \"Based on the Delphi of GANs approach and established thresholds, \"\n",
        "    explanation += f\"this model is considered {overall_bias}. \"\n",
        "    explanation += \" \".join(interpretations)\n",
        "\n",
        "    return overall_bias, explanation"
      ],
      "metadata": {
        "id": "LIs8yvXXHX4D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Starting bias detection process...\")\n",
        "\n",
        "    download_datasets()\n",
        "\n",
        "    X_gender, y_gender = load_data('gender_bias_data.csv', 'gender_bias')\n",
        "    X_racial, y_racial = load_data('racial_bias_data.csv', 'racial_bias')\n",
        "    X_age, y_age = load_data('age_bias_data.csv', 'age_bias')\n",
        "\n",
        "    low_bias_model, high_bias_model, bert_tokenizer, distilbert_tokenizer = load_models()\n",
        "\n",
        "    bert_output_shape = low_bias_model.config.hidden_size\n",
        "    distilbert_output_shape = high_bias_model.config.hidden_size\n",
        "\n",
        "    gans = {}\n",
        "    for bias_type, X in [('gender', X_gender), ('racial', X_racial), ('age', X_age)]:\n",
        "        print(f\"\\nTraining GAN for {bias_type} bias...\")\n",
        "        generator, discriminator = create_gan(X.shape[1], max(bert_output_shape, distilbert_output_shape))\n",
        "        train_gan(generator, discriminator, X)\n",
        "        gans[bias_type] = (generator, discriminator)\n",
        "\n",
        "    test_sentences = [\n",
        "        \"The doctor performed the surgery.\",\n",
        "        \"The nurse took care of the patient.\",\n",
        "        \"The engineer designed the bridge.\",\n",
        "        \"The teacher explained the lesson.\",\n",
        "        \"The CEO made a crucial decision.\",\n",
        "        \"The immigrant started a successful business.\",\n",
        "        \"The elderly person learned to use a smartphone.\",\n",
        "        \"The young adult bought their first house.\",\n",
        "        \"The politician addressed the diverse crowd.\",\n",
        "        \"The artist created a controversial piece.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nDetecting bias for BERT (relatively low-bias model)...\")\n",
        "    bert_scores = detect_bias(low_bias_model, bert_tokenizer, gans, test_sentences)\n",
        "    bert_bias_level, bert_explanation = interpret_bias_scores(bert_scores)\n",
        "\n",
        "    print(\"\\nDetecting bias for DistilBERT (potentially higher-bias model)...\")\n",
        "    distilbert_scores = detect_bias(high_bias_model, distilbert_tokenizer, gans, test_sentences)\n",
        "    distilbert_bias_level, distilbert_explanation = interpret_bias_scores(distilbert_scores)\n",
        "\n",
        "    print(\"\\nFinal Bias Detection Results:\")\n",
        "    print(\"\\nBERT Model (bert-base-uncased):\")\n",
        "    for gan_name, score in bert_scores:\n",
        "        print(f\"{gan_name.capitalize()} Bias Score: {score:.4f}\")\n",
        "    print(f\"\\nOverall Bias Level: {bert_bias_level}\")\n",
        "    print(\"Explanation:\", bert_explanation)\n",
        "\n",
        "    print(\"\\nDistilBERT Model (distilbert-base-uncased):\")\n",
        "    for gan_name, score in distilbert_scores:\n",
        "        print(f\"{gan_name.capitalize()} Bias Score: {score:.4f}\")\n",
        "    print(f\"\\nOverall Bias Level: {distilbert_bias_level}\")\n",
        "    print(\"Explanation:\", distilbert_explanation)\n",
        "\n",
        "    print(\"\\nBias detection process completed.\")"
      ],
      "metadata": {
        "id": "FUYBa9-uHcm8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nklxddbCHekw",
        "outputId": "5f99c7e0-7c87-49c9-dce8-8c4aa996f154"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bias detection process...\n",
            "gender_bias dataset already exists.\n",
            "racial_bias dataset already exists.\n",
            "age_bias dataset already exists.\n",
            "Loading and preprocessing gender_bias data...\n",
            "gender_bias data preprocessed. Shape: (540, 20)\n",
            "Loading and preprocessing racial_bias data...\n",
            "racial_bias data preprocessed. Shape: (1000, 20)\n",
            "Loading and preprocessing age_bias data...\n",
            "age_bias data preprocessed. Shape: (299, 11)\n",
            "Loading pre-trained models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GAN for gender bias...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/1000 [00:29<3:19:41, 12.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, D Loss: 0.6764, G Loss: 9.9358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 102/1000 [00:40<01:41,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, D Loss: 0.0001, G Loss: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 202/1000 [00:51<02:11,  6.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200, D Loss: 0.0001, G Loss: 0.0043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 302/1000 [01:02<01:49,  6.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 300, D Loss: 0.0000, G Loss: 0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 402/1000 [01:15<01:02,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 400, D Loss: 0.0000, G Loss: 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 502/1000 [01:28<00:53,  9.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, D Loss: 0.0000, G Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 602/1000 [01:40<00:41,  9.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 600, D Loss: 0.0000, G Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 701/1000 [01:51<00:29, 10.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 700, D Loss: 0.0000, G Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 803/1000 [02:03<00:19, 10.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 800, D Loss: 0.0000, G Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 901/1000 [02:13<00:09, 10.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 900, D Loss: 0.0000, G Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:24<00:00,  6.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GAN for racial bias...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/1000 [00:03<22:28,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, D Loss: 0.7741, G Loss: 9.0343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 102/1000 [00:14<01:31,  9.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, D Loss: 0.0003, G Loss: 0.0093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 202/1000 [00:26<01:26,  9.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200, D Loss: 0.0001, G Loss: 0.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 302/1000 [00:36<01:38,  7.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 300, D Loss: 0.0000, G Loss: 0.0007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 402/1000 [00:48<00:58, 10.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 400, D Loss: 0.0001, G Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 503/1000 [01:00<00:49, 10.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, D Loss: 0.0000, G Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 601/1000 [01:11<00:39, 10.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 600, D Loss: 0.0000, G Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 702/1000 [01:23<00:30,  9.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 700, D Loss: 0.0000, G Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 803/1000 [01:33<00:19, 10.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 800, D Loss: 0.0000, G Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 902/1000 [01:44<00:11,  8.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 900, D Loss: 0.0000, G Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:54<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GAN for age bias...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:03<1:06:26,  3.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, D Loss: 0.7275, G Loss: 8.7011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 102/1000 [00:16<01:37,  9.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, D Loss: 0.0002, G Loss: 0.0060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 203/1000 [00:28<01:52,  7.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200, D Loss: 0.0001, G Loss: 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 302/1000 [00:40<01:55,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 300, D Loss: 0.0000, G Loss: 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 402/1000 [00:51<01:19,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 400, D Loss: 0.0000, G Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 501/1000 [01:05<00:51,  9.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, D Loss: 0.0000, G Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 602/1000 [01:16<00:41,  9.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 600, D Loss: 0.0000, G Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 702/1000 [01:28<00:31,  9.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 700, D Loss: 0.0000, G Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 802/1000 [01:39<00:21,  9.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 800, D Loss: 0.0000, G Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 902/1000 [01:51<00:10,  9.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 900, D Loss: 0.0000, G Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:02<00:00,  8.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detecting bias for BERT (relatively low-bias model)...\n",
            "1/1 [==============================] - 0s 420ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7bcb27ce09d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 455ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 1007 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7bcb22375090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 434ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "Detecting bias for DistilBERT (potentially higher-bias model)...\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "Final Bias Detection Results:\n",
            "\n",
            "BERT Model (bert-base-uncased):\n",
            "Gender Bias Score: 0.9174\n",
            "Racial Bias Score: 0.9108\n",
            "Age Bias Score: 0.7801\n",
            "\n",
            "Overall Bias Level: biased\n",
            "Explanation: Based on the Delphi of GANs approach and established thresholds, this model is considered biased. Gender bias: Significantly biased (score: 0.9174) Racial bias: Significantly biased (score: 0.9108) Age bias: Significantly biased (score: 0.7801)\n",
            "\n",
            "DistilBERT Model (distilbert-base-uncased):\n",
            "Gender Bias Score: 0.9170\n",
            "Racial Bias Score: 0.9131\n",
            "Age Bias Score: 0.7982\n",
            "\n",
            "Overall Bias Level: biased\n",
            "Explanation: Based on the Delphi of GANs approach and established thresholds, this model is considered biased. Gender bias: Significantly biased (score: 0.9170) Racial bias: Significantly biased (score: 0.9131) Age bias: Significantly biased (score: 0.7982)\n",
            "\n",
            "Bias detection process completed.\n"
          ]
        }
      ]
    }
  ]
}